---
title: "FML Assignment 5"
author: "Roopali Aggarwal"
date: "2024-04-04"
output: html_document
---
Instructions -

The purpose of this assignment is to use Hierarchical Clustering
The dataset Cereals.csv includes nutritional information, store display, and consumer ratings for 77 breakfast cereals.
Data Preprocessing. Remove all cereals with missing values.
```{r}
library(dplyr)
library(tidyverse)
library(caret)
library(factoextra)
library(flexclust)
library(cluster)
library(ISLR)
library(ggplot2)
library(lattice)
library(knitr)
#Read the data using the working directory
Cereals.data <- read.csv("/Applications/Cereals.csv")
#Summary of the data, showing dimensions and structure
summary(Cereals.data)
str(Cereals.data)
head(Cereals.data)
```
```{r}
#Removing rows with missing values
data <- na.omit(Cereals.data)
#Extracting only the numerical variables
num.data <- data[, c(4:16)]
num.data
#Standardizing the dataset
data.norm <- preProcess(num.data , method = c("center", "scale"))
df <- predict(data.norm , num.data)
df
```
Following pre-processing and scaling, the total count of observations amounted to 74, as opposed to the initial 77. Specifically, only three records were identified with the value "NA".

Question 1 - Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward. Choose the best method.
```{r}
#Dissimilarity matrix for all the numerical value using Euclidean method
distance <- dist(df, method = "euclidean")
#Perform hierarchical clustering using Agnes with different linkage methods and plotting the outcome
#Single Linkage Method
hc_single <- agnes(df,method = "single")
plot(hc_single, main = "AGNES Single Linkage Method for Customer Cereal Rating", xlab = "Cereal", ylab = "Height", cex.axis = 1, cex = 0.50)
```
```{r}
#Complete Linkage Method
hc_complete <- agnes(df,method = "complete")
plot(hc_complete, main = "AGNES Complete Linkage Method for Customer Cereal Rating",   xlab = "Cereal", ylab = "Height", cex.axis = 1, cex = 0.50)
```
```{r}
#Average Linkage Method
hc_average <- agnes(df,method = "average")
plot(hc_average, main = "AGNES Average Linkage Method for Customer Cereal Rating", xlab = "Cereal", ylab = "Height", cex.axis = 1, cex = 0.50)
```
```{r}
#Ward Linkage Method
hc_ward <- agnes(df,method = "ward")
plot(hc_ward, main = "AGNES Ward Linkage Method for Customer Cereal Rating", xlab = "Cereal", ylab = "Height", cex.axis = 1, cex = 0.50)
```

The clustering method with values approaching 1.0 signifies a refined structure, guiding the selection process. Among the tested options, Single Linkage scored 0.61, Complete Linkage 0.84, and Average Linkage 0.78, while the Ward Linkage stood out with a score of 0.90. Consequently, the data indicates that the Ward technique is the most effective clustering strategy. It produced clusters where cereals within each group exhibited high similarity and clear distinction from cereals in other groups, aligning with the goal of grouping similar cereals together while ensuring distinctiveness among groups. Hence, Ward's method emerges as the optimal choice, providing the clearest and most distinct cereal groupings.

Question 2 - How many clusters would you choose?
```{r}
#Finding the numbers of clusters using silhouette and elbow method
fviz_nbclust(df, hcut, method = "silhouette", k.max = 15) +
 labs(title = "Silhouette Method")
fviz_nbclust(df, hcut, method = "wss", k.max = 15) + labs(title = "Elbow Method") + 
  geom_vline(xintercept = 12, linetype=2)
```
The findings from both the silhouette and elbow methods suggest that the ideal number of clusters is 12.
```{r}
#Plotting the Dendogram
plot(hc_ward, main = "Dendogram", xlab = "Cereal", ylab = "Height", cex.axis = 1, cex = 0.50)
rect.hclust(hc_ward, k = 12, border = 1:12)
```

Based on the Dendogram, there would be 12 clusters

Question 3 - Comment on the structure of the clusters and on their stability. Hint: To check stability,partition the data and see how well clusters formed based on one part apply to the other part. To do this:
● Cluster partition A
● Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid).
● Assess how consistent the cluster assignments are compared to the assignments based on all the data.

```{r}
clust1 <- cutree(hc_ward, k=12)
y3 <- as.data.frame(cbind(df,clust1))
```

```{r}
#Partitioning into group A & group B
set.seed(123)
newdata <- Cereals.data
new.df <- na.omit(newdata)
train_data <- new.df[1:55,]
test_data <- new.df[56:74,]
#Conducting hierarchical clustering with a consideration for K = 12
hc_ward1 <- agnes(scale(train_data[,-c(1:3)]),method = "ward")
hc_average1 <- agnes(scale(train_data[,-c(1:3)]),method="average")
hc_complete1 <- agnes(scale(train_data[,-c(1:3)]),method="complete")
hc_single1 <- agnes(scale(train_data[,-c(1:3)]),method="single")
kable(cbind(ward=hc_ward1$ac,average=hc_average1$ac,complete=hc_complete$ac,single=hc_single1$ac))
```

```{r}
pltree(hc_ward1, cex = 0.6, hang = -1, main = "Dendogram of Agnes")
rect.hclust(hc_ward1, k = 12, border = 2:5)
```
```{r}
clust2 <- cutree(hc_ward1, k=12)
result <- as.data.frame(cbind(scale(train_data[,-c(1:3)]),clust2))
result[result$clust2==1,]
```
```{r}
#First Centroid
center1 <- colMeans(result[result$clust2==1,])
result[result$clust2==2,]
```
```{r}
#Second Centroid
center2<-colMeans(result[result$clust2==2,])
result[result$clust2==3,]
```
```{r}
#Third Centroid
center3<-colMeans(result[result$clust2==3,])
result[result$clust2==4,]
```
```{r}
#Fourth Centroid
center4<-colMeans(result[result$clust2==4,])
centers<-rbind(center1,center2,center3,center4)
#Calculating the distance
x2 <- as.data.frame(rbind(centers[,-14],scale(test_data[,-c(1:3)])))
y1 <- get_dist(x2)
y2 <- as.matrix(y1)
d1 <- data.frame(data=seq(1,nrow(test_data),1),clusters=rep(0,nrow(test_data)))
for(i in 1:nrow(test_data))
{
  d1[i,2]<-which.min(y2[i+4,1:4])
}
d1
```
```{r}
cbind(y3$clust1[56:74],d1$clusters)
```
```{r}
table(y3$clust1[56:74]==d1$clusters)
```
Model stability: Demonstrates high instability owing to the scarcity of "TRUE" values.

Question 4 - The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?

```{r}
#Grouping "Nutritious Cereals"
healthy.df <- df[, c("calories", "protein", "fat", "fiber", "carbo", "sugars", "potass", "vitamins")]
#Dissimilarity matrix using euclidean method
distance <- dist(healthy.df, method = "euclidean")
hc <- hclust(distance, method = "ward.D2")
#Creating 12 clusters as value of k is 12
df$cluster <- cutree(hc, k = 12)
summary.df <- aggregate(. ~ cluster, data = df, FUN = mean)
summary.df
```
Based on the provided cluster information, we can identify the best cluster for children as the one with the most favorable nutritional attributes. From the given data, Cluster 1 appears to be the healthiest option for children. Here's the explanation:

Low Calories: Cluster 1 has the lowest calorie content compared to other clusters, which is beneficial for maintaining a healthy weight and avoiding excessive calorie intake.

High Protein: Protein is essential for growth and development, and Cluster 1 has the highest protein content among all clusters, providing the necessary building blocks for muscles and tissues.

Low Fat and Sugar: Cluster 1 also has relatively low fat and sugar content, which is crucial for preventing obesity, diabetes, and other related health issues.

High Fiber: Fiber is important for digestive health and helps regulate blood sugar levels. Cluster 1 exhibits the highest fiber content, promoting satiety and aiding in digestion.

Balanced Nutrients: Overall, Cluster 1 demonstrates a balanced nutritional profile with adequate levels of vitamins and minerals, such as potassium and vitamins, essential for overall health and well-being.

Therefore, Cluster 1 represents the healthiest choice for children, offering a combination of low-calorie, high-protein, low-fat, low-sugar, and high-fiber cereals that contribute to a nutritious diet conducive to growth, development, and overall health.

Here, we have already done the normalizing on the data above, so there's no need to normalize it again. This ensures that the comparison between clusters is based on standardized values, providing a fair assessment of their nutritional quality. 